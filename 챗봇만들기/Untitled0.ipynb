{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EtAkjFVKonZu271sd-vkDgtpP7Y-0wwX","authorship_tag":"ABX9TyPSWTsEvL0ZLf9mhmp8whnu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow as tf\n","from tensorflow import keras\n","import re"],"metadata":{"id":"3y6T7YNxGfv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbNxLuI_C6hT"},"outputs":[],"source":["data = pd.read_csv('/content/drive/MyDrive/딥러닝 스터디/챗봇만들기/Conversation.csv')"]},{"cell_type":"code","source":["data.tail(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148},"id":"YA4qKHRvGkjY","executionInfo":{"status":"ok","timestamp":1681667630237,"user_tz":-540,"elapsed":3,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"5cd9563e-5444-4498-80aa-6ee5f9888dff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0                                           question  \\\n","3722        3722                                  yes. all my life.   \n","3723        3723  you're wearing out your right hand. stop using...   \n","3724        3724        but i do all my writing with my right hand.   \n","\n","                                                 answer  \n","3722  you're wearing out your right hand. stop using...  \n","3723        but i do all my writing with my right hand.  \n","3724  start typing instead. that way your left hand ...  "],"text/html":["\n","  <div id=\"df-c1f16c8e-47ea-4867-bc7d-d23a5144a564\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>question</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3722</th>\n","      <td>3722</td>\n","      <td>yes. all my life.</td>\n","      <td>you're wearing out your right hand. stop using...</td>\n","    </tr>\n","    <tr>\n","      <th>3723</th>\n","      <td>3723</td>\n","      <td>you're wearing out your right hand. stop using...</td>\n","      <td>but i do all my writing with my right hand.</td>\n","    </tr>\n","    <tr>\n","      <th>3724</th>\n","      <td>3724</td>\n","      <td>but i do all my writing with my right hand.</td>\n","      <td>start typing instead. that way your left hand ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1f16c8e-47ea-4867-bc7d-d23a5144a564')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c1f16c8e-47ea-4867-bc7d-d23a5144a564 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c1f16c8e-47ea-4867-bc7d-d23a5144a564');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["contractions = {\"'cause\": 'because',\n"," \"I'd\": 'I would',\n"," \"I'd've\": 'I would have',\n"," \"I'll\": 'I will',\n"," \"I'll've\": 'I will have',\n"," \"I'm\": 'I am',\n"," \"I've\": 'I have',\n"," \"ain't\": 'is not',\n"," \"aren't\": 'are not',\n"," \"can't\": 'cannot',\n"," \"could've\": 'could have',\n"," \"couldn't\": 'could not',\n"," \"didn't\": 'did not',\n"," \"doesn't\": 'does not',\n"," \"don't\": 'do not',\n"," \"hadn't\": 'had not',\n"," \"hasn't\": 'has not',\n"," \"haven't\": 'have not',\n"," \"he'd\": 'he would',\n"," \"he'll\": 'he will',\n"," \"he's\": 'he is',\n"," \"here's\": 'here is',\n"," \"how'd\": 'how did',\n"," \"how'd'y\": 'how do you',\n"," \"how'll\": 'how will',\n"," \"how's\": 'how is',\n"," \"i'd\": 'i would',\n"," \"i'd've\": 'i would have',\n"," \"i'll\": 'i will',\n"," \"i'll've\": 'i will have',\n"," \"i'm\": 'i am',\n"," \"i've\": 'i have',\n"," \"isn't\": 'is not',\n"," \"it'd\": 'it would',\n"," \"it'd've\": 'it would have',\n"," \"it'll\": 'it will',\n"," \"it'll've\": 'it will have',\n"," \"it's\": 'it is',\n"," \"let's\": 'let us',\n"," \"ma'am\": 'madam',\n"," \"mayn't\": 'may not',\n"," \"might've\": 'might have',\n"," \"mightn't\": 'might not',\n"," \"mightn't've\": 'might not have',\n"," \"must've\": 'must have',\n"," \"mustn't\": 'must not',\n"," \"mustn't've\": 'must not have',\n"," \"needn't\": 'need not',\n"," \"needn't've\": 'need not have',\n"," \"o'clock\": 'of the clock',\n"," \"oughtn't\": 'ought not',\n"," \"oughtn't've\": 'ought not have',\n"," \"sha'n't\": 'shall not',\n"," \"shan't\": 'shall not',\n"," \"shan't've\": 'shall not have',\n"," \"she'd\": 'she would',\n"," \"she'd've\": 'she would have',\n"," \"she'll\": 'she will',\n"," \"she'll've\": 'she will have',\n"," \"she's\": 'she is',\n"," \"should've\": 'should have',\n"," \"shouldn't\": 'should not',\n"," \"shouldn't've\": 'should not have',\n"," \"so's\": 'so as',\n"," \"so've\": 'so have',\n"," \"that'd\": 'that would',\n"," \"that'd've\": 'that would have',\n"," \"that's\": 'that is',\n"," \"there'd\": 'there would',\n"," \"there'd've\": 'there would have',\n"," \"there's\": 'there is',\n"," \"they'd\": 'they would',\n"," \"they'd've\": 'they would have',\n"," \"they'll\": 'they will',\n"," \"they'll've\": 'they will have',\n"," \"they're\": 'they are',\n"," \"they've\": 'they have',\n"," \"this's\": 'this is',\n"," \"to've\": 'to have',\n"," \"wasn't\": 'was not',\n"," \"we'd\": 'we would',\n"," \"we'd've\": 'we would have',\n"," \"we'll\": 'we will',\n"," \"we'll've\": 'we will have',\n"," \"we're\": 'we are',\n"," \"we've\": 'we have',\n"," \"weren't\": 'were not',\n"," \"what'll\": 'what will',\n"," \"what'll've\": 'what will have',\n"," \"what're\": 'what are',\n"," \"what's\": 'what is',\n"," \"what've\": 'what have',\n"," \"when's\": 'when is',\n"," \"when've\": 'when have',\n"," \"where'd\": 'where did',\n"," \"where's\": 'where is',\n"," \"where've\": 'where have',\n"," \"who'll\": 'who will',\n"," \"who'll've\": 'who will have',\n"," \"who's\": 'who is',\n"," \"who've\": 'who have',\n"," \"why's\": 'why is',\n"," \"why've\": 'why have',\n"," \"will've\": 'will have',\n"," \"won't\": 'will not',\n"," \"won't've\": 'will not have',\n"," \"would've\": 'would have',\n"," \"wouldn't\": 'would not',\n"," \"wouldn't've\": 'would not have',\n"," \"y'all\": 'you all',\n"," \"y'all'd\": 'you all would',\n"," \"y'all'd've\": 'you all would have',\n"," \"y'all're\": 'you all are',\n"," \"y'all've\": 'you all have',\n"," \"you'd\": 'you would',\n"," \"you'd've\": 'you would have',\n"," \"you'll\": 'you will',\n"," \"you'll've\": 'you will have',\n"," \"you're\": 'you are',\n"," \"you've\": 'you have'}"],"metadata":{"id":"kKYpQ03ynkeF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['question'][4]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"53n14DRenU62","executionInfo":{"status":"ok","timestamp":1681667638514,"user_tz":-540,"elapsed":279,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"63daf4cb-03ed-41e9-a422-345605352db1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"i've been great. what about you?\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["data.isna().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IcRvxJ2zGuFP","executionInfo":{"status":"ok","timestamp":1681667639796,"user_tz":-540,"elapsed":1,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"be126004-1266-4347-9c23-7d19cc57a4af"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Unnamed: 0    0\n","question      0\n","answer        0\n","dtype: int64"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["def clean_text(sentence):\n","    sentence = sentence.lower()\n","    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n","    sentence = re.sub('\"','', sentence)\n","    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")])\n","    sentence = re.sub(r\"'s\\b\",\"\",sentence)\n","    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n","\n","    return sentence"],"metadata":{"id":"DpxiqJUknn3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['question'] = data['question'].apply(clean_text)\n","data['answer'] = data['answer'].apply(clean_text)"],"metadata":{"id":"-_CzhIJ2G2Uy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['question'][4]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"pJs2ii5JoZVt","executionInfo":{"status":"ok","timestamp":1681667645746,"user_tz":-540,"elapsed":2,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"79e527e2-8c04-4f06-eac9-04fa82275341"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'i have been great  what about you '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# encoder_input = []\n","# decoder_input = []\n","# decoder_output = []\n","# for sent in data['question']:\n","#     encoder_input.append(sent.split())\n","\n","# for sent in data['answer']:\n","#     decoder_input.append(('<sos> ' + sent).split())\n","\n","# for sent in data['answer']:\n","#     decoder_output.append((sent + ' <eos>').split())    "],"metadata":{"id":"NK-rMcMuI9Eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.replace('', np.nan, inplace=True)\n","print(data.isnull().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7qtRkUXoo_M","executionInfo":{"status":"ok","timestamp":1681665308739,"user_tz":-540,"elapsed":2,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"742dd1da-ca37-4af9-81a1-34744f250991"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unnamed: 0    0\n","question      0\n","answer        0\n","dtype: int64\n"]}]},{"cell_type":"code","source":["question_len = [len(s.split()) for s in data['question']]\n","answer_len = [len(s.split()) for s in data['answer']]\n","\n","print('질문 최소 길이 : {}'.format(np.min(question_len)))\n","print('질문 최대 길이 : {}'.format(np.max(question_len)))\n","print('질문 평균 길이 : {}'.format(np.mean(question_len)))\n","\n","print('답변 최소 길이 : {}'.format(np.min(answer_len)))\n","print('답변 최대 길이 : {}'.format(np.max(answer_len)))\n","print('답변 평균 길이 : {}'.format(np.mean(answer_len)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U0bkdrxDow7_","executionInfo":{"status":"ok","timestamp":1681667653813,"user_tz":-540,"elapsed":266,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"7a2dd606-9a51-4da1-8ae0-b28538ea0623"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["질문 최소 길이 : 0\n","질문 최대 길이 : 20\n","질문 평균 길이 : 6.667114093959731\n","답변 최소 길이 : 0\n","답변 최대 길이 : 20\n","답변 평균 길이 : 6.859328859060403\n"]}]},{"cell_type":"code","source":["data['decoder_input'] = data['answer'].apply(lambda x : 'starttoken '+ x)\n","data['decoder_output'] = data['answer'].apply(lambda x : x + ' endtoken')"],"metadata":{"id":"i11NAgYipJ31"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_input = np.array(data['question'])\n","decoder_input = np.array(data['decoder_input'])\n","decoder_output = np.array(data['decoder_output'])"],"metadata":{"id":"gVrQYB9bpWff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoder_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_A6QkXjAEYbw","executionInfo":{"status":"ok","timestamp":1681667664900,"user_tz":-540,"elapsed":411,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"ddd320c6-4f6a-4b79-a3ba-01418d95c792"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['i am fine  how about yourself  endtoken',\n","       'i am pretty good  thanks for asking  endtoken',\n","       'no problem  so how have you been  endtoken', ...,\n","       'you are wearing out your right hand  stop using it so much  endtoken',\n","       'but i do all my writing with my right hand  endtoken',\n","       'start typing instead  that way your left hand will do half the work  endtoken'],\n","      dtype=object)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["tokenizer_qu = Tokenizer()\n","tokenizer_qu.fit_on_texts(encoder_input)\n","encoder_input = tokenizer_qu.texts_to_sequences(encoder_input)"],"metadata":{"id":"vuKDwIunKdKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer_an = Tokenizer()\n","tokenizer_an.fit_on_texts(decoder_input)\n","tokenizer_an.fit_on_texts(decoder_output)\n","\n","decoder_input = tokenizer_an.texts_to_sequences(decoder_input)\n","decoder_output = tokenizer_an.texts_to_sequences(decoder_output)"],"metadata":{"id":"whq43tQ2y0KM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_input = pad_sequences(encoder_input, padding='post', maxlen= 21)\n","decoder_input = pad_sequences(decoder_input, padding='post', maxlen= 21)\n","decoder_output = pad_sequences(decoder_output, padding='post', maxlen= 21)"],"metadata":{"id":"z9w3DFEyFTQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(encoder_input[0])\n","print(decoder_input[0])\n","print(decoder_output[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ng5kkwXQM-qx","executionInfo":{"status":"ok","timestamp":1681667675206,"user_tz":-540,"elapsed":1,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"c319f514-0245-46bb-e599-442fe334730d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[961  35  12   2 148   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0]\n","[  1   3  32 521  38  35 522   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0]\n","[  3  32 521  38  35 522   2   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0]\n"]}]},{"cell_type":"code","source":["encoder_input.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-YjFNKaGDWo","executionInfo":{"status":"ok","timestamp":1681667683111,"user_tz":-540,"elapsed":776,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"e16b0c4f-5de6-4647-acf7-a4f74de804b7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3725, 21)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","encoder_input_train, encoder_input_test , decoder_input_train, decoder_input_test , decoder_output_train, decoder_output_test = train_test_split(encoder_input,decoder_input, decoder_output, test_size= 0.2, shuffle= True )"],"metadata":{"id":"dWE5Y6X5Fk5E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking, Concatenate\n","from tensorflow.keras.models import Model"],"metadata":{"id":"jYRhSI5vrH2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keras.backend.clear_session()\n","\n","embedding_dim = 32\n","hidden_size = 32\n","\n","# 인코더\n","encoder_inputs = Input(shape=(21,))\n","\n","# 인코더의 임베딩 층\n","encoder_embed = Embedding(len(tokenizer_qu.word_index)+1, embedding_dim)(encoder_inputs)\n","\n","# 인코더의 마스킹킹\n","encoder_mask = Masking(mask_value=0)(encoder_embed)\n","\n","#LSTM \n","encoder_outputs, h_state, c_state = LSTM(50, return_state=True, return_sequences=True)(encoder_mask)"],"metadata":{"id":"qMVR7TzbNaW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoder_inputs = Input(shape=(21,))\n","decoder_embed = Embedding(len(tokenizer_an.word_index)+1, 50)(decoder_inputs)\n","decoder_mask = Masking(mask_value=0)(decoder_embed)\n","decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_mask, initial_state=[h_state, c_state])"],"metadata":{"id":"1Za83AOukgAy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","\n","logger = tf.get_logger()\n","\n","class AttentionLayer(tf.keras.layers.Layer):\n","    \"\"\"\n","    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n","    There are three sets of weights introduced W_a, U_a, and V_a\n","     \"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super(AttentionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert isinstance(input_shape, list)\n","        # Create a trainable weight variable for this layer.\n","\n","        self.W_a = self.add_weight(name='W_a',\n","                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n","                                   initializer='uniform',\n","                                   trainable=True)\n","        self.U_a = self.add_weight(name='U_a',\n","                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n","                                   initializer='uniform',\n","                                   trainable=True)\n","        self.V_a = self.add_weight(name='V_a',\n","                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n","                                   initializer='uniform',\n","                                   trainable=True)\n","\n","        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n","\n","    def call(self, inputs):\n","        \"\"\"\n","        inputs: [encoder_output_sequence, decoder_output_sequence]\n","        \"\"\"\n","        assert type(inputs) == list\n","        encoder_out_seq, decoder_out_seq = inputs\n","\n","        logger.debug(f\"encoder_out_seq.shape = {encoder_out_seq.shape}\")\n","        logger.debug(f\"decoder_out_seq.shape = {decoder_out_seq.shape}\")\n","\n","        def energy_step(inputs, states):\n","            \"\"\" Step function for computing energy for a single decoder state\n","            inputs: (batchsize * 1 * de_in_dim)\n","            states: (batchsize * 1 * de_latent_dim)\n","            \"\"\"\n","\n","            logger.debug(\"Running energy computation step\")\n","\n","            if not isinstance(states, (list, tuple)):\n","                raise TypeError(f\"States must be an iterable. Got {states} of type {type(states)}\")\n","\n","            encoder_full_seq = states[-1]\n","\n","            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n","            # <= batch size * en_seq_len * latent_dim\n","            W_a_dot_s = K.dot(encoder_full_seq, self.W_a)\n","\n","            \"\"\" Computing hj.Ua \"\"\"\n","            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n","\n","            logger.debug(f\"U_a_dot_h.shape = {U_a_dot_h.shape}\")\n","\n","            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n","            # <= batch_size*en_seq_len, latent_dim\n","            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n","\n","            logger.debug(f\"Ws_plus_Uh.shape = {Ws_plus_Uh.shape}\")\n","\n","            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n","            # <= batch_size, en_seq_len\n","            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n","            # <= batch_size, en_seq_len\n","            e_i = K.softmax(e_i)\n","\n","            logger.debug(f\"ei.shape = {e_i.shape}\")\n","\n","            return e_i, [e_i]\n","\n","        def context_step(inputs, states):\n","            \"\"\" Step function for computing ci using ei \"\"\"\n","\n","            logger.debug(\"Running attention vector computation step\")\n","\n","            if not isinstance(states, (list, tuple)):\n","                raise TypeError(f\"States must be an iterable. Got {states} of type {type(states)}\")\n","\n","            encoder_full_seq = states[-1]\n","\n","            # <= batch_size, hidden_size\n","            c_i = K.sum(encoder_full_seq * K.expand_dims(inputs, -1), axis=1)\n","\n","            logger.debug(f\"ci.shape = {c_i.shape}\")\n","\n","            return c_i, [c_i]\n","\n","        # we don't maintain states between steps when computing attention\n","        # attention is stateless, so we're passing a fake state for RNN step function\n","        fake_state_c = K.sum(encoder_out_seq, axis=1)\n","        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n","\n","        \"\"\" Computing energy outputs \"\"\"\n","        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n","        last_out, e_outputs, _ = K.rnn(\n","            energy_step, decoder_out_seq, [fake_state_e], constants=[encoder_out_seq]\n","        )\n","\n","        \"\"\" Computing context vectors \"\"\"\n","        last_out, c_outputs, _ = K.rnn(\n","            context_step, e_outputs, [fake_state_c], constants=[encoder_out_seq]\n","        )\n","\n","        return c_outputs, e_outputs\n","\n","    def compute_output_shape(self, input_shape):\n","        \"\"\" Outputs produced by the layer \"\"\"\n","        return [\n","            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n","            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n","        ]"],"metadata":{"id":"Mmd58WbpGpFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attn_layer = AttentionLayer()\n","attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n","decoder_concat_input = Concatenate()([decoder_outputs, attn_out])\n","\n","decoder_dense = Dense(len(tokenizer_an.word_index)+1, activation='softmax')\n","decoder_softmax_outputs = decoder_dense(decoder_concat_input)"],"metadata":{"id":"X7GZS1zYsVRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n","model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n","model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_output_train, validation_data = ([encoder_input_test, decoder_input_test], decoder_output_test), batch_size = 128, epochs = 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lj2Pmvo0e_vN","executionInfo":{"status":"ok","timestamp":1681670052572,"user_tz":-540,"elapsed":1113283,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"ef2d4f75-e14f-4237-f0a0-c96d385047f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","24/24 [==============================] - 28s 680ms/step - loss: 1.6862 - acc: 0.7081 - val_loss: 1.8128 - val_acc: 0.7062\n","Epoch 2/100\n","24/24 [==============================] - 10s 403ms/step - loss: 1.6759 - acc: 0.7090 - val_loss: 1.8184 - val_acc: 0.7066\n","Epoch 3/100\n","24/24 [==============================] - 11s 463ms/step - loss: 1.6726 - acc: 0.7097 - val_loss: 1.8106 - val_acc: 0.7058\n","Epoch 4/100\n","24/24 [==============================] - 11s 467ms/step - loss: 1.6681 - acc: 0.7095 - val_loss: 1.8082 - val_acc: 0.7066\n","Epoch 5/100\n","24/24 [==============================] - 11s 481ms/step - loss: 1.6645 - acc: 0.7098 - val_loss: 1.8070 - val_acc: 0.7063\n","Epoch 6/100\n","24/24 [==============================] - 9s 366ms/step - loss: 1.6607 - acc: 0.7100 - val_loss: 1.8050 - val_acc: 0.7066\n","Epoch 7/100\n","24/24 [==============================] - 11s 463ms/step - loss: 1.6567 - acc: 0.7105 - val_loss: 1.8156 - val_acc: 0.7060\n","Epoch 8/100\n","24/24 [==============================] - 11s 464ms/step - loss: 1.6524 - acc: 0.7111 - val_loss: 1.7973 - val_acc: 0.7074\n","Epoch 9/100\n","24/24 [==============================] - 12s 510ms/step - loss: 1.6483 - acc: 0.7113 - val_loss: 1.8061 - val_acc: 0.7070\n","Epoch 10/100\n","24/24 [==============================] - 10s 413ms/step - loss: 1.6449 - acc: 0.7112 - val_loss: 1.7977 - val_acc: 0.7078\n","Epoch 11/100\n","24/24 [==============================] - 10s 414ms/step - loss: 1.6415 - acc: 0.7118 - val_loss: 1.8014 - val_acc: 0.7082\n","Epoch 12/100\n","24/24 [==============================] - 11s 456ms/step - loss: 1.6368 - acc: 0.7126 - val_loss: 1.7982 - val_acc: 0.7079\n","Epoch 13/100\n","24/24 [==============================] - 11s 465ms/step - loss: 1.6345 - acc: 0.7126 - val_loss: 1.7983 - val_acc: 0.7080\n","Epoch 14/100\n","24/24 [==============================] - 10s 426ms/step - loss: 1.6303 - acc: 0.7128 - val_loss: 1.7905 - val_acc: 0.7079\n","Epoch 15/100\n","24/24 [==============================] - 10s 407ms/step - loss: 1.6269 - acc: 0.7135 - val_loss: 1.7892 - val_acc: 0.7087\n","Epoch 16/100\n","24/24 [==============================] - 11s 468ms/step - loss: 1.6228 - acc: 0.7133 - val_loss: 1.7885 - val_acc: 0.7075\n","Epoch 17/100\n","24/24 [==============================] - 11s 464ms/step - loss: 1.6197 - acc: 0.7137 - val_loss: 1.8030 - val_acc: 0.7059\n","Epoch 18/100\n","24/24 [==============================] - 11s 453ms/step - loss: 1.6154 - acc: 0.7142 - val_loss: 1.7859 - val_acc: 0.7089\n","Epoch 19/100\n","24/24 [==============================] - 9s 391ms/step - loss: 1.6123 - acc: 0.7143 - val_loss: 1.7921 - val_acc: 0.7074\n","Epoch 20/100\n","24/24 [==============================] - 11s 461ms/step - loss: 1.6090 - acc: 0.7146 - val_loss: 1.7942 - val_acc: 0.7065\n","Epoch 21/100\n","24/24 [==============================] - 11s 466ms/step - loss: 1.6054 - acc: 0.7149 - val_loss: 1.7886 - val_acc: 0.7092\n","Epoch 22/100\n","24/24 [==============================] - 11s 451ms/step - loss: 1.6019 - acc: 0.7154 - val_loss: 1.7850 - val_acc: 0.7081\n","Epoch 23/100\n","24/24 [==============================] - 9s 386ms/step - loss: 1.5985 - acc: 0.7156 - val_loss: 1.7808 - val_acc: 0.7086\n","Epoch 24/100\n","24/24 [==============================] - 11s 459ms/step - loss: 1.5945 - acc: 0.7156 - val_loss: 1.7836 - val_acc: 0.7094\n","Epoch 25/100\n","24/24 [==============================] - 11s 464ms/step - loss: 1.5912 - acc: 0.7159 - val_loss: 1.7825 - val_acc: 0.7096\n","Epoch 26/100\n","24/24 [==============================] - 11s 460ms/step - loss: 1.5883 - acc: 0.7163 - val_loss: 1.7769 - val_acc: 0.7097\n","Epoch 27/100\n","24/24 [==============================] - 9s 383ms/step - loss: 1.5848 - acc: 0.7163 - val_loss: 1.7839 - val_acc: 0.7070\n","Epoch 28/100\n","24/24 [==============================] - 11s 461ms/step - loss: 1.5804 - acc: 0.7167 - val_loss: 1.7756 - val_acc: 0.7090\n","Epoch 29/100\n","24/24 [==============================] - 11s 466ms/step - loss: 1.5780 - acc: 0.7171 - val_loss: 1.7758 - val_acc: 0.7088\n","Epoch 30/100\n","24/24 [==============================] - 11s 468ms/step - loss: 1.5739 - acc: 0.7174 - val_loss: 1.7769 - val_acc: 0.7096\n","Epoch 31/100\n","24/24 [==============================] - 11s 447ms/step - loss: 1.5709 - acc: 0.7176 - val_loss: 1.7728 - val_acc: 0.7100\n","Epoch 32/100\n","24/24 [==============================] - 10s 396ms/step - loss: 1.5684 - acc: 0.7179 - val_loss: 1.7692 - val_acc: 0.7110\n","Epoch 33/100\n","24/24 [==============================] - 11s 467ms/step - loss: 1.5639 - acc: 0.7184 - val_loss: 1.7696 - val_acc: 0.7103\n","Epoch 34/100\n","24/24 [==============================] - 11s 459ms/step - loss: 1.5614 - acc: 0.7185 - val_loss: 1.7715 - val_acc: 0.7105\n","Epoch 35/100\n","24/24 [==============================] - 12s 489ms/step - loss: 1.5584 - acc: 0.7188 - val_loss: 1.7759 - val_acc: 0.7090\n","Epoch 36/100\n","24/24 [==============================] - 9s 392ms/step - loss: 1.5556 - acc: 0.7190 - val_loss: 1.7720 - val_acc: 0.7091\n","Epoch 37/100\n","24/24 [==============================] - 11s 451ms/step - loss: 1.5521 - acc: 0.7191 - val_loss: 1.7672 - val_acc: 0.7101\n","Epoch 38/100\n","24/24 [==============================] - 11s 457ms/step - loss: 1.5485 - acc: 0.7193 - val_loss: 1.7655 - val_acc: 0.7104\n","Epoch 39/100\n","24/24 [==============================] - 11s 463ms/step - loss: 1.5452 - acc: 0.7199 - val_loss: 1.7731 - val_acc: 0.7109\n","Epoch 40/100\n","24/24 [==============================] - 11s 453ms/step - loss: 1.5422 - acc: 0.7199 - val_loss: 1.7617 - val_acc: 0.7109\n","Epoch 41/100\n","24/24 [==============================] - 10s 393ms/step - loss: 1.5386 - acc: 0.7205 - val_loss: 1.7621 - val_acc: 0.7107\n","Epoch 42/100\n","24/24 [==============================] - 11s 464ms/step - loss: 1.5359 - acc: 0.7204 - val_loss: 1.7661 - val_acc: 0.7105\n","Epoch 43/100\n","24/24 [==============================] - 11s 468ms/step - loss: 1.5332 - acc: 0.7209 - val_loss: 1.7639 - val_acc: 0.7107\n","Epoch 44/100\n","24/24 [==============================] - 11s 444ms/step - loss: 1.5297 - acc: 0.7213 - val_loss: 1.7642 - val_acc: 0.7098\n","Epoch 45/100\n","24/24 [==============================] - 10s 403ms/step - loss: 1.5262 - acc: 0.7216 - val_loss: 1.7682 - val_acc: 0.7116\n","Epoch 46/100\n","24/24 [==============================] - 11s 469ms/step - loss: 1.5253 - acc: 0.7216 - val_loss: 1.7631 - val_acc: 0.7108\n","Epoch 47/100\n","24/24 [==============================] - 11s 469ms/step - loss: 1.5208 - acc: 0.7222 - val_loss: 1.7649 - val_acc: 0.7090\n","Epoch 48/100\n","24/24 [==============================] - 11s 468ms/step - loss: 1.5170 - acc: 0.7225 - val_loss: 1.7601 - val_acc: 0.7112\n","Epoch 49/100\n","24/24 [==============================] - 9s 391ms/step - loss: 1.5152 - acc: 0.7223 - val_loss: 1.7656 - val_acc: 0.7095\n","Epoch 50/100\n","24/24 [==============================] - 11s 440ms/step - loss: 1.5119 - acc: 0.7229 - val_loss: 1.7667 - val_acc: 0.7112\n","Epoch 51/100\n","24/24 [==============================] - 11s 458ms/step - loss: 1.5089 - acc: 0.7227 - val_loss: 1.7621 - val_acc: 0.7112\n","Epoch 52/100\n","24/24 [==============================] - 11s 459ms/step - loss: 1.5068 - acc: 0.7233 - val_loss: 1.7662 - val_acc: 0.7112\n","Epoch 53/100\n","24/24 [==============================] - 10s 411ms/step - loss: 1.5033 - acc: 0.7235 - val_loss: 1.7613 - val_acc: 0.7096\n","Epoch 54/100\n","24/24 [==============================] - 10s 420ms/step - loss: 1.5003 - acc: 0.7238 - val_loss: 1.7598 - val_acc: 0.7117\n","Epoch 55/100\n","24/24 [==============================] - 11s 466ms/step - loss: 1.5000 - acc: 0.7236 - val_loss: 1.7618 - val_acc: 0.7119\n","Epoch 56/100\n","24/24 [==============================] - 11s 466ms/step - loss: 1.4953 - acc: 0.7241 - val_loss: 1.7631 - val_acc: 0.7122\n","Epoch 57/100\n","24/24 [==============================] - 11s 459ms/step - loss: 1.4916 - acc: 0.7243 - val_loss: 1.7622 - val_acc: 0.7113\n","Epoch 58/100\n","24/24 [==============================] - 10s 396ms/step - loss: 1.4901 - acc: 0.7247 - val_loss: 1.7781 - val_acc: 0.7080\n","Epoch 59/100\n","24/24 [==============================] - 11s 460ms/step - loss: 1.4862 - acc: 0.7250 - val_loss: 1.7688 - val_acc: 0.7118\n","Epoch 60/100\n","24/24 [==============================] - 11s 463ms/step - loss: 1.4839 - acc: 0.7250 - val_loss: 1.7646 - val_acc: 0.7101\n","Epoch 61/100\n","24/24 [==============================] - 12s 496ms/step - loss: 1.4815 - acc: 0.7251 - val_loss: 1.7649 - val_acc: 0.7096\n","Epoch 62/100\n","24/24 [==============================] - 11s 453ms/step - loss: 1.4776 - acc: 0.7256 - val_loss: 1.7630 - val_acc: 0.7112\n","Epoch 63/100\n","24/24 [==============================] - 9s 386ms/step - loss: 1.4767 - acc: 0.7261 - val_loss: 1.7646 - val_acc: 0.7110\n","Epoch 64/100\n","24/24 [==============================] - 10s 440ms/step - loss: 1.4726 - acc: 0.7260 - val_loss: 1.7599 - val_acc: 0.7110\n","Epoch 65/100\n","24/24 [==============================] - 11s 461ms/step - loss: 1.4717 - acc: 0.7261 - val_loss: 1.7618 - val_acc: 0.7112\n","Epoch 66/100\n","24/24 [==============================] - 10s 434ms/step - loss: 1.4683 - acc: 0.7262 - val_loss: 1.7610 - val_acc: 0.7122\n","Epoch 67/100\n","24/24 [==============================] - 10s 400ms/step - loss: 1.4652 - acc: 0.7265 - val_loss: 1.7668 - val_acc: 0.7089\n","Epoch 68/100\n","24/24 [==============================] - 10s 430ms/step - loss: 1.4627 - acc: 0.7265 - val_loss: 1.7617 - val_acc: 0.7108\n","Epoch 69/100\n","24/24 [==============================] - 11s 456ms/step - loss: 1.4604 - acc: 0.7266 - val_loss: 1.7655 - val_acc: 0.7120\n","Epoch 70/100\n","24/24 [==============================] - 10s 440ms/step - loss: 1.4580 - acc: 0.7272 - val_loss: 1.7664 - val_acc: 0.7117\n","Epoch 71/100\n","24/24 [==============================] - 9s 385ms/step - loss: 1.4554 - acc: 0.7272 - val_loss: 1.7650 - val_acc: 0.7113\n","Epoch 72/100\n","24/24 [==============================] - 11s 468ms/step - loss: 1.4520 - acc: 0.7277 - val_loss: 1.7671 - val_acc: 0.7124\n","Epoch 73/100\n","24/24 [==============================] - 11s 465ms/step - loss: 1.4507 - acc: 0.7279 - val_loss: 1.7744 - val_acc: 0.7076\n","Epoch 74/100\n","24/24 [==============================] - 11s 485ms/step - loss: 1.4474 - acc: 0.7279 - val_loss: 1.7658 - val_acc: 0.7103\n","Epoch 75/100\n","24/24 [==============================] - 9s 392ms/step - loss: 1.4454 - acc: 0.7286 - val_loss: 1.7695 - val_acc: 0.7103\n","Epoch 76/100\n","24/24 [==============================] - 11s 454ms/step - loss: 1.4430 - acc: 0.7284 - val_loss: 1.7692 - val_acc: 0.7112\n","Epoch 77/100\n","24/24 [==============================] - 11s 468ms/step - loss: 1.4404 - acc: 0.7285 - val_loss: 1.7664 - val_acc: 0.7097\n","Epoch 78/100\n","24/24 [==============================] - 11s 461ms/step - loss: 1.4384 - acc: 0.7285 - val_loss: 1.7636 - val_acc: 0.7121\n","Epoch 79/100\n","24/24 [==============================] - 9s 382ms/step - loss: 1.4347 - acc: 0.7288 - val_loss: 1.7729 - val_acc: 0.7112\n","Epoch 80/100\n","24/24 [==============================] - 11s 442ms/step - loss: 1.4326 - acc: 0.7291 - val_loss: 1.7663 - val_acc: 0.7107\n","Epoch 81/100\n","24/24 [==============================] - 11s 462ms/step - loss: 1.4302 - acc: 0.7297 - val_loss: 1.7671 - val_acc: 0.7111\n","Epoch 82/100\n","24/24 [==============================] - 11s 469ms/step - loss: 1.4285 - acc: 0.7296 - val_loss: 1.7687 - val_acc: 0.7099\n","Epoch 83/100\n","24/24 [==============================] - 11s 451ms/step - loss: 1.4258 - acc: 0.7298 - val_loss: 1.7712 - val_acc: 0.7081\n","Epoch 84/100\n","24/24 [==============================] - 9s 388ms/step - loss: 1.4242 - acc: 0.7308 - val_loss: 1.7702 - val_acc: 0.7096\n","Epoch 85/100\n","24/24 [==============================] - 11s 464ms/step - loss: 1.4206 - acc: 0.7302 - val_loss: 1.7748 - val_acc: 0.7118\n","Epoch 86/100\n","24/24 [==============================] - 11s 470ms/step - loss: 1.4187 - acc: 0.7297 - val_loss: 1.7693 - val_acc: 0.7108\n","Epoch 87/100\n","24/24 [==============================] - 12s 496ms/step - loss: 1.4160 - acc: 0.7309 - val_loss: 1.7672 - val_acc: 0.7094\n","Epoch 88/100\n","24/24 [==============================] - 9s 392ms/step - loss: 1.4138 - acc: 0.7307 - val_loss: 1.7683 - val_acc: 0.7112\n","Epoch 89/100\n","24/24 [==============================] - 11s 448ms/step - loss: 1.4113 - acc: 0.7305 - val_loss: 1.7686 - val_acc: 0.7091\n","Epoch 90/100\n","24/24 [==============================] - 11s 462ms/step - loss: 1.4091 - acc: 0.7319 - val_loss: 1.7681 - val_acc: 0.7085\n","Epoch 91/100\n","24/24 [==============================] - 11s 459ms/step - loss: 1.4070 - acc: 0.7314 - val_loss: 1.7725 - val_acc: 0.7101\n","Epoch 92/100\n","24/24 [==============================] - 11s 452ms/step - loss: 1.4039 - acc: 0.7321 - val_loss: 1.7797 - val_acc: 0.7089\n","Epoch 93/100\n","24/24 [==============================] - 10s 395ms/step - loss: 1.4020 - acc: 0.7326 - val_loss: 1.7750 - val_acc: 0.7104\n","Epoch 94/100\n","24/24 [==============================] - 11s 462ms/step - loss: 1.4004 - acc: 0.7319 - val_loss: 1.7756 - val_acc: 0.7121\n","Epoch 95/100\n","24/24 [==============================] - 11s 459ms/step - loss: 1.3976 - acc: 0.7325 - val_loss: 1.7767 - val_acc: 0.7126\n","Epoch 96/100\n","24/24 [==============================] - 11s 455ms/step - loss: 1.3957 - acc: 0.7326 - val_loss: 1.7736 - val_acc: 0.7090\n","Epoch 97/100\n","24/24 [==============================] - 9s 377ms/step - loss: 1.3923 - acc: 0.7326 - val_loss: 1.7765 - val_acc: 0.7108\n","Epoch 98/100\n","24/24 [==============================] - 11s 464ms/step - loss: 1.3920 - acc: 0.7326 - val_loss: 1.7777 - val_acc: 0.7094\n","Epoch 99/100\n","24/24 [==============================] - 11s 462ms/step - loss: 1.3886 - acc: 0.7334 - val_loss: 1.7779 - val_acc: 0.7101\n","Epoch 100/100\n","24/24 [==============================] - 12s 499ms/step - loss: 1.3858 - acc: 0.7336 - val_loss: 1.7867 - val_acc: 0.7092\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fc215c4a2e0>"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["encoder_model = Model(encoder_inputs, [encoder_outputs, h_state, c_state])"],"metadata":{"id":"pv6f8YZdG3r3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_h_state = Input(shape=(50,))\n","encoder_c_state = Input(shape=(50,))\n","\n","pd_decoder_outputs, pd_h_state, pd_c_state = decoder_lstm(decoder_mask, initial_state=[encoder_h_state, encoder_c_state])\n","\n","\n","pd_encoder_outputs = Input(shape=(21, 50))\n","pd_attn_out, pd_attn_states = attn_layer([pd_encoder_outputs, pd_decoder_outputs])\n","pd_decoder_concat = Concatenate()([pd_decoder_outputs, pd_attn_out])\n","\n","pd_decoder_softmax_outputs = decoder_dense(pd_decoder_concat)\n","\n","decoder_model = Model([decoder_inputs, pd_encoder_outputs, encoder_h_state, encoder_c_state], [pd_decoder_softmax_outputs, pd_h_state, pd_c_state])"],"metadata":{"id":"eiSugLEsG3px"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["an_to_index = tokenizer_an.word_index\n","index_to_an = tokenizer_an.index_word"],"metadata":{"id":"v1MSx8QTHHiy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer_an.word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rugELB0LUbA","executionInfo":{"status":"ok","timestamp":1681670078155,"user_tz":-540,"elapsed":481,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"18f80d0f-5825-475b-dddc-5408535e2e3e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'starttoken': 1,\n"," 'endtoken': 2,\n"," 'i': 3,\n"," 'you': 4,\n"," 'the': 5,\n"," 'is': 6,\n"," 'it': 7,\n"," 'to': 8,\n"," 'a': 9,\n"," 'that': 10,\n"," 'not': 11,\n"," 'do': 12,\n"," 'what': 13,\n"," 'are': 14,\n"," 'have': 15,\n"," 'will': 16,\n"," 'of': 17,\n"," 'and': 18,\n"," 'they': 19,\n"," 'did': 20,\n"," 'he': 21,\n"," 'in': 22,\n"," 'was': 23,\n"," 'so': 24,\n"," 'yes': 25,\n"," 'we': 26,\n"," 'like': 27,\n"," 'for': 28,\n"," 'no': 29,\n"," 'but': 30,\n"," 'be': 31,\n"," 'am': 32,\n"," 'me': 33,\n"," 'my': 34,\n"," 'about': 35,\n"," 'on': 36,\n"," 'too': 37,\n"," 'how': 38,\n"," 'go': 39,\n"," 'think': 40,\n"," 'would': 41,\n"," 'why': 42,\n"," 'your': 43,\n"," 'good': 44,\n"," 'there': 45,\n"," 'going': 46,\n"," 'get': 47,\n"," 'all': 48,\n"," 'with': 49,\n"," 'well': 50,\n"," 'want': 51,\n"," 'really': 52,\n"," 'know': 53,\n"," 'one': 54,\n"," 'just': 55,\n"," 'at': 56,\n"," 'she': 57,\n"," 'can': 58,\n"," 'should': 59,\n"," 'if': 60,\n"," 'people': 61,\n"," 'this': 62,\n"," 'then': 63,\n"," 'right': 64,\n"," 'see': 65,\n"," 'maybe': 66,\n"," 'does': 67,\n"," 'oh': 68,\n"," 'out': 69,\n"," 'her': 70,\n"," 'time': 71,\n"," 'money': 72,\n"," 'nice': 73,\n"," 'us': 74,\n"," 'him': 75,\n"," 'okay': 76,\n"," 'course': 77,\n"," 'need': 78,\n"," 'when': 79,\n"," 'or': 80,\n"," 'lot': 81,\n"," 'much': 82,\n"," 'because': 83,\n"," 'only': 84,\n"," 'up': 85,\n"," 'sure': 86,\n"," 'day': 87,\n"," 'got': 88,\n"," 'them': 89,\n"," 'make': 90,\n"," 'an': 91,\n"," 'now': 92,\n"," 'cannot': 93,\n"," 'mean': 94,\n"," 'been': 95,\n"," 'who': 96,\n"," 'let': 97,\n"," 'had': 98,\n"," 'school': 99,\n"," 'from': 100,\n"," 'take': 101,\n"," 'next': 102,\n"," 'something': 103,\n"," 'new': 104,\n"," 'great': 105,\n"," 'every': 106,\n"," 'more': 107,\n"," 'our': 108,\n"," 'some': 109,\n"," 'love': 110,\n"," 'say': 111,\n"," 'look': 112,\n"," 'his': 113,\n"," 'never': 114,\n"," 'where': 115,\n"," 'movie': 116,\n"," 'has': 117,\n"," 'give': 118,\n"," 'were': 119,\n"," 'better': 120,\n"," 'hope': 121,\n"," 'here': 122,\n"," 'off': 123,\n"," 'tell': 124,\n"," 'thank': 125,\n"," 'any': 126,\n"," 'said': 127,\n"," 'come': 128,\n"," 'fun': 129,\n"," 'buy': 130,\n"," 'old': 131,\n"," 'two': 132,\n"," 'their': 133,\n"," 'very': 134,\n"," 'today': 135,\n"," 'sounds': 136,\n"," 'job': 137,\n"," 'could': 138,\n"," 't': 139,\n"," 'eat': 140,\n"," 'someone': 141,\n"," 'put': 142,\n"," 'work': 143,\n"," 'other': 144,\n"," 'food': 145,\n"," 'bad': 146,\n"," 'yeah': 147,\n"," 'weather': 148,\n"," 'always': 149,\n"," 'even': 150,\n"," 'anything': 151,\n"," 'idea': 152,\n"," 'back': 153,\n"," 'big': 154,\n"," 'into': 155,\n"," 'long': 156,\n"," 'ever': 157,\n"," 'many': 158,\n"," 'car': 159,\n"," 'as': 160,\n"," 'pretty': 161,\n"," 'problem': 162,\n"," 'than': 163,\n"," 'over': 164,\n"," 'might': 165,\n"," 'still': 166,\n"," 'little': 167,\n"," 'nothing': 168,\n"," 'feel': 169,\n"," 'house': 170,\n"," 'party': 171,\n"," 'tv': 172,\n"," 'world': 173,\n"," 'clean': 174,\n"," 'went': 175,\n"," 'best': 176,\n"," 'use': 177,\n"," 'nose': 178,\n"," 'doing': 179,\n"," 'wish': 180,\n"," 'wait': 181,\n"," 'heard': 182,\n"," 'tomorrow': 183,\n"," 'first': 184,\n"," 'home': 185,\n"," 'told': 186,\n"," 'find': 187,\n"," 'by': 188,\n"," 'after': 189,\n"," 'same': 190,\n"," 'sorry': 191,\n"," 'talk': 192,\n"," 'hard': 193,\n"," 'thought': 194,\n"," 'years': 195,\n"," 'matter': 196,\n"," 'restaurant': 197,\n"," 'minutes': 198,\n"," 'thanks': 199,\n"," 'believe': 200,\n"," 'different': 201,\n"," 'five': 202,\n"," 'kind': 203,\n"," 'game': 204,\n"," 'guess': 205,\n"," 'everyone': 206,\n"," 'rain': 207,\n"," 'hot': 208,\n"," 'down': 209,\n"," 'smell': 210,\n"," 'most': 211,\n"," 'things': 212,\n"," 'call': 213,\n"," 'yet': 214,\n"," 'thing': 215,\n"," 'before': 216,\n"," 'bought': 217,\n"," 'another': 218,\n"," 'happened': 219,\n"," 'news': 220,\n"," 'show': 221,\n"," 'stop': 222,\n"," 'else': 223,\n"," 'looking': 224,\n"," 'gets': 225,\n"," 'night': 226,\n"," 'phone': 227,\n"," 'already': 228,\n"," 'happy': 229,\n"," 'those': 230,\n"," 'friend': 231,\n"," 'start': 232,\n"," 'lots': 233,\n"," 'until': 234,\n"," 'wanted': 235,\n"," 'eyes': 236,\n"," 'mine': 237,\n"," 'watch': 238,\n"," 'last': 239,\n"," 'year': 240,\n"," 'care': 241,\n"," 'water': 242,\n"," 'later': 243,\n"," 'says': 244,\n"," 'change': 245,\n"," 'talking': 246,\n"," 'around': 247,\n"," 'wrong': 248,\n"," 'these': 249,\n"," 'cost': 250,\n"," 'while': 251,\n"," 'man': 252,\n"," 'close': 253,\n"," 'must': 254,\n"," 'gave': 255,\n"," 'dinner': 256,\n"," 'left': 257,\n"," 'save': 258,\n"," 'cold': 259,\n"," 'life': 260,\n"," 'busy': 261,\n"," 'seen': 262,\n"," 'again': 263,\n"," 'help': 264,\n"," 'keep': 265,\n"," 'three': 266,\n"," 'hate': 267,\n"," 'enough': 268,\n"," 'cars': 269,\n"," 'away': 270,\n"," 'dogs': 271,\n"," 'woman': 272,\n"," 'police': 273,\n"," 'started': 274,\n"," 'sometimes': 275,\n"," 'looks': 276,\n"," 'way': 277,\n"," 'perfect': 278,\n"," 'reason': 279,\n"," 'probably': 280,\n"," 'took': 281,\n"," 'week': 282,\n"," 'real': 283,\n"," 'class': 284,\n"," 'once': 285,\n"," 'movies': 286,\n"," 'hurt': 287,\n"," 'english': 288,\n"," 'pounds': 289,\n"," 'catch': 290,\n"," 'friday': 291,\n"," 'family': 292,\n"," 'cheese': 293,\n"," 'forever': 294,\n"," 'yesterday': 295,\n"," 'bet': 296,\n"," 'kids': 297,\n"," 'run': 298,\n"," 'ticket': 299,\n"," 'play': 300,\n"," 'which': 301,\n"," 'everything': 302,\n"," 'soon': 303,\n"," 'rains': 304,\n"," 'stay': 305,\n"," 'thinking': 306,\n"," 'light': 307,\n"," 'sick': 308,\n"," 'shoes': 309,\n"," 'saw': 310,\n"," 'whole': 311,\n"," 'made': 312,\n"," 'free': 313,\n"," 'glad': 314,\n"," 'hear': 315,\n"," 'having': 316,\n"," 'try': 317,\n"," 'city': 318,\n"," 'rich': 319,\n"," 'friends': 320,\n"," 'turn': 321,\n"," 'fish': 322,\n"," 'each': 323,\n"," 'dirty': 324,\n"," 'bathroom': 325,\n"," 'easy': 326,\n"," 'cut': 327,\n"," 'knows': 328,\n"," 'date': 329,\n"," 'makes': 330,\n"," 'without': 331,\n"," 'doctor': 332,\n"," 'four': 333,\n"," 'especially': 334,\n"," 'outside': 335,\n"," 'cool': 336,\n"," 'enjoy': 337,\n"," 'true': 338,\n"," 'deal': 339,\n"," 'weekend': 340,\n"," 'beach': 341,\n"," 'supposed': 342,\n"," 'times': 343,\n"," 'feet': 344,\n"," 'couple': 345,\n"," 'getting': 346,\n"," 'please': 347,\n"," 'walk': 348,\n"," 'mom': 349,\n"," 'dollar': 350,\n"," 'almost': 351,\n"," 'leave': 352,\n"," 'terrible': 353,\n"," 'coffee': 354,\n"," 'cat': 355,\n"," 'used': 356,\n"," 'fat': 357,\n"," 'street': 358,\n"," 'problems': 359,\n"," 'cigarettes': 360,\n"," 'teach': 361,\n"," 'story': 362,\n"," 'read': 363,\n"," 'room': 364,\n"," 'travel': 365,\n"," 'cigarette': 366,\n"," 'may': 367,\n"," 'winter': 368,\n"," 'understand': 369,\n"," 'beautiful': 370,\n"," 'store': 371,\n"," 'ago': 372,\n"," 'found': 373,\n"," 'high': 374,\n"," 'funny': 375,\n"," 'came': 376,\n"," 'music': 377,\n"," 'listen': 378,\n"," 'both': 379,\n"," 'won': 380,\n"," 'lost': 381,\n"," 'baby': 382,\n"," 'god': 383,\n"," 'visit': 384,\n"," 'didn': 385,\n"," 'also': 386,\n"," 'eight': 387,\n"," 'live': 388,\n"," 'neither': 389,\n"," 'jokes': 390,\n"," 'dog': 391,\n"," 'pizza': 392,\n"," 'means': 393,\n"," 'percent': 394,\n"," 'costs': 395,\n"," 'black': 396,\n"," 'lose': 397,\n"," 'looked': 398,\n"," 'bread': 399,\n"," 'smart': 400,\n"," 'hour': 401,\n"," 'forget': 402,\n"," 'using': 403,\n"," 'till': 404,\n"," 'check': 405,\n"," 'students': 406,\n"," 'bring': 407,\n"," 'hand': 408,\n"," 'white': 409,\n"," 'dangerous': 410,\n"," 'butter': 411,\n"," 'ball': 412,\n"," 'vote': 413,\n"," 'lately': 414,\n"," 'far': 415,\n"," 'exactly': 416,\n"," 'minute': 417,\n"," 'called': 418,\n"," 'own': 419,\n"," 'favorite': 420,\n"," 'through': 421,\n"," 'end': 422,\n"," 'such': 423,\n"," 'working': 424,\n"," 'invitation': 425,\n"," 'clock': 426,\n"," 'fast': 427,\n"," 'ready': 428,\n"," 'girlfriend': 429,\n"," 'hit': 430,\n"," 'goes': 431,\n"," 'sunday': 432,\n"," 'asked': 433,\n"," 'dad': 434,\n"," 'plane': 435,\n"," 'full': 436,\n"," 'wonder': 437,\n"," 'late': 438,\n"," 'digital': 439,\n"," 'boy': 440,\n"," 'fix': 441,\n"," 'price': 442,\n"," 'number': 443,\n"," 'win': 444,\n"," 'blue': 445,\n"," 'waiter': 446,\n"," 'eating': 447,\n"," 'orange': 448,\n"," 'president': 449,\n"," 'asking': 450,\n"," 'actually': 451,\n"," 'middle': 452,\n"," 'rather': 453,\n"," 'fresh': 454,\n"," 'seems': 455,\n"," 'california': 456,\n"," 'happen': 457,\n"," 'mind': 458,\n"," 'girl': 459,\n"," 'feeling': 460,\n"," 'earlier': 461,\n"," 'serious': 462,\n"," 'draw': 463,\n"," 'paint': 464,\n"," 'loved': 465,\n"," 'either': 466,\n"," 'team': 467,\n"," 'tried': 468,\n"," 'whatever': 469,\n"," 'crazy': 470,\n"," 'waiting': 471,\n"," 'starts': 472,\n"," 'gives': 473,\n"," 'm': 474,\n"," 'hold': 475,\n"," 'worry': 476,\n"," 'pasadena': 477,\n"," 'wash': 478,\n"," 'guys': 479,\n"," 'likes': 480,\n"," 'pay': 481,\n"," 'divorced': 482,\n"," 'together': 483,\n"," 'schools': 484,\n"," 'friendly': 485,\n"," 'everybody': 486,\n"," 'finish': 487,\n"," 'open': 488,\n"," 'hungry': 489,\n"," 'homework': 490,\n"," 'grow': 491,\n"," 'paper': 492,\n"," 'ask': 493,\n"," 'pick': 494,\n"," 'fruit': 495,\n"," 'wake': 496,\n"," 'blow': 497,\n"," 'few': 498,\n"," 'men': 499,\n"," 'numbers': 500,\n"," 'shopping': 501,\n"," 'card': 502,\n"," 'front': 503,\n"," 'remember': 504,\n"," 'wipe': 505,\n"," 'hire': 506,\n"," 'war': 507,\n"," 'college': 508,\n"," 'sale': 509,\n"," 'half': 510,\n"," 'driver': 511,\n"," 'seats': 512,\n"," 'tires': 513,\n"," 'delicious': 514,\n"," 'table': 515,\n"," 'gun': 516,\n"," 'laid': 517,\n"," 'potatoes': 518,\n"," 'taxes': 519,\n"," 'power': 520,\n"," 'fine': 521,\n"," 'yourself': 522,\n"," 'raining': 523,\n"," 'though': 524,\n"," 'air': 525,\n"," 'sky': 526,\n"," 'easier': 527,\n"," 'trying': 528,\n"," 'tall': 529,\n"," 'telling': 530,\n"," 'under': 531,\n"," 'promotion': 532,\n"," 'outfit': 533,\n"," 'myself': 534,\n"," 'pair': 535,\n"," 'learn': 536,\n"," 'superbad': 537,\n"," 'dvd': 538,\n"," 'line': 539,\n"," 'watching': 540,\n"," 'nobody': 541,\n"," 'question': 542,\n"," 'missing': 543,\n"," 'o': 544,\n"," 'given': 545,\n"," 'stand': 546,\n"," 'nosey': 547,\n"," 'polite': 548,\n"," 'being': 549,\n"," 'million': 550,\n"," 'lunch': 551,\n"," 'month': 552,\n"," 'anymore': 553,\n"," 'fell': 554,\n"," 'agree': 555,\n"," 'bottom': 556,\n"," 'wear': 557,\n"," 'helped': 558,\n"," 'envelope': 559,\n"," 'stamp': 560,\n"," 'kitchen': 561,\n"," 'mail': 562,\n"," 'cream': 563,\n"," 'face': 564,\n"," 'rude': 565,\n"," 'heaven': 566,\n"," 'die': 567,\n"," 'parents': 568,\n"," 'lesson': 569,\n"," 'father': 570,\n"," 'button': 571,\n"," 'sheets': 572,\n"," 'drive': 573,\n"," 'loud': 574,\n"," 'tissue': 575,\n"," 'age': 576,\n"," 'worried': 577,\n"," 'afford': 578,\n"," 'carry': 579,\n"," 'pocket': 580,\n"," 'pants': 581,\n"," 'lucky': 582,\n"," 'comes': 583,\n"," 'flag': 584,\n"," 'pockets': 585,\n"," 'parking': 586,\n"," 'safe': 587,\n"," 'hours': 588,\n"," 'blinking': 589,\n"," 'tree': 590,\n"," 'baseball': 591,\n"," 'online': 592,\n"," 'singers': 593,\n"," 'blind': 594,\n"," 'drugs': 595,\n"," 'steak': 596,\n"," 'hands': 597,\n"," 'cheaper': 598,\n"," 'crime': 599,\n"," 'smoking': 600,\n"," 'building': 601,\n"," 'mayor': 602,\n"," 'doctors': 603,\n"," 'salt': 604,\n"," 'milk': 605,\n"," 'voted': 606,\n"," 'luck': 607,\n"," 'absolutely': 608,\n"," 'summer': 609,\n"," 'rained': 610,\n"," 'stars': 611,\n"," 'clearly': 612,\n"," 'warm': 613,\n"," 'trip': 614,\n"," 'plan': 615,\n"," 'hundred': 616,\n"," 'haven': 617,\n"," 'stomach': 618,\n"," 'medicine': 619,\n"," 'days': 620,\n"," 'art': 621,\n"," 'talented': 622,\n"," 'talent': 623,\n"," 'liked': 624,\n"," 'laughing': 625,\n"," 'listening': 626,\n"," 'kinds': 627,\n"," 'b': 628,\n"," 'type': 629,\n"," 'exciting': 630,\n"," 'score': 631,\n"," 'gone': 632,\n"," 'assignments': 633,\n"," 'anyone': 634,\n"," 'wow': 635,\n"," 'switch': 636,\n"," 'anyway': 637,\n"," 'alone': 638,\n"," 'instead': 639,\n"," 'visited': 640,\n"," 'invite': 641,\n"," 'morning': 642,\n"," 'throwing': 643,\n"," 'coming': 644,\n"," 'business': 645,\n"," 'don': 646,\n"," 'empty': 647,\n"," 'wife': 648,\n"," 'interesting': 649,\n"," 'houses': 650,\n"," 'bit': 651,\n"," 'slow': 652,\n"," 'shop': 653,\n"," 'head': 654,\n"," 'loves': 655,\n"," 'expensive': 656,\n"," 'penny': 657,\n"," 'miles': 658,\n"," 'spend': 659,\n"," 'animals': 660,\n"," 'sitting': 661,\n"," 'cares': 662,\n"," 'bed': 663,\n"," 'record': 664,\n"," 'church': 665,\n"," 'puppy': 666,\n"," 'shots': 667,\n"," 'buried': 668,\n"," 'battery': 669,\n"," 'laptop': 670,\n"," 'cents': 671,\n"," 'drink': 672,\n"," 'radio': 673,\n"," 'cuts': 674,\n"," 'fridge': 675,\n"," 'sandwich': 676,\n"," 'paying': 677,\n"," 'bluedog': 678,\n"," 'book': 679,\n"," 'channels': 680,\n"," 'six': 681,\n"," 'u': 682,\n"," 'landed': 683,\n"," 'worth': 684,\n"," 'point': 685,\n"," 'beer': 686,\n"," 'pen': 687,\n"," 'hole': 688,\n"," 'patch': 689,\n"," 'ten': 690,\n"," 'ones': 691,\n"," 'songs': 692,\n"," 'whenever': 693,\n"," 'takes': 694,\n"," 'space': 695,\n"," 'mud': 696,\n"," 'carpet': 697,\n"," 'case': 698,\n"," 'teacher': 699,\n"," 'writing': 700,\n"," 'killed': 701,\n"," 'thieves': 702,\n"," 'husband': 703,\n"," 'died': 704,\n"," 'hood': 705,\n"," 'fire': 706,\n"," 'person': 707,\n"," 'isn': 708,\n"," 'red': 709,\n"," 'sign': 710,\n"," 'hurry': 711,\n"," 'accident': 712,\n"," 'park': 713,\n"," 'order': 714,\n"," 'jail': 715,\n"," 'players': 716,\n"," 'thinks': 717,\n"," 'move': 718,\n"," 'quit': 719,\n"," 'running': 720,\n"," 'town': 721,\n"," 'early': 722,\n"," 'ears': 723,\n"," 'french': 724,\n"," 'tomato': 725,\n"," 'nails': 726,\n"," 'stains': 727,\n"," 'peanuts': 728,\n"," 'hospitals': 729,\n"," 'smoke': 730,\n"," 'fires': 731,\n"," 'fall': 732,\n"," 'pets': 733,\n"," 'airport': 734,\n"," 'break': 735,\n"," 'ice': 736,\n"," 'ii': 737,\n"," 'hamburgers': 738,\n"," 'pepper': 739,\n"," 'cows': 740,\n"," 'ate': 741,\n"," 'banana': 742,\n"," 'pink': 743,\n"," 'shirt': 744,\n"," 'solve': 745,\n"," 'voting': 746,\n"," 'stress': 747,\n"," 'lie': 748,\n"," 'government': 749,\n"," 'pcc': 750,\n"," 'recently': 751,\n"," 'weird': 752,\n"," 'ninety': 753,\n"," 'seem': 754,\n"," 'smells': 755,\n"," 'unpredictable': 756,\n"," 'sooner': 757,\n"," 'plans': 758,\n"," 'hang': 759,\n"," 'seeing': 760,\n"," 'apologize': 761,\n"," 'brown': 762,\n"," 'short': 763,\n"," 'girls': 764,\n"," 'quite': 765,\n"," 'stayed': 766,\n"," 'excited': 767,\n"," 'congratulations': 768,\n"," 'appreciate': 769,\n"," 'macy': 770,\n"," 'forty': 771,\n"," 'dollars': 772,\n"," 'chucks': 773,\n"," 'learned': 774,\n"," 'knew': 775,\n"," 'sort': 776,\n"," 'brought': 777,\n"," 'super': 778,\n"," 'rock': 779,\n"," 'r': 780,\n"," 'instruments': 781,\n"," 'missed': 782,\n"," 'none': 783,\n"," 'pass': 784,\n"," 'eye': 785,\n"," 'able': 786,\n"," 'return': 787,\n"," 'miss': 788,\n"," 'ounces': 789,\n"," 'cute': 790,\n"," 'answer': 791,\n"," 'stopping': 792,\n"," 'sound': 793,\n"," 'p': 794,\n"," 'saturday': 795,\n"," 'parties': 796,\n"," 'rush': 797,\n"," 'sometime': 798,\n"," 'set': 799,\n"," 'broke': 800,\n"," 'dive': 801,\n"," 'turned': 802,\n"," 'joking': 803,\n"," 'small': 804,\n"," 'wonderful': 805,\n"," 'comfortable': 806,\n"," 'drinking': 807,\n"," 'salad': 808,\n"," 'seven': 809,\n"," 'mouth': 810,\n"," 'meant': 811,\n"," 'letter': 812,\n"," 'asleep': 813,\n"," 'others': 814,\n"," 'cats': 815,\n"," 'stuff': 816,\n"," 'hmm': 817,\n"," 'needs': 818,\n"," 'vegetables': 819,\n"," 'cell': 820,\n"," 'stole': 821,\n"," 'mcdonald': 822,\n"," 'born': 823,\n"," 'extra': 824,\n"," 'towels': 825,\n"," 'dry': 826,\n"," 'tax': 827,\n"," 'market': 828,\n"," 'apples': 829,\n"," 'ham': 830,\n"," 'crashed': 831,\n"," 'plus': 832,\n"," 'dream': 833,\n"," 'son': 834,\n"," 'meet': 835,\n"," 'everywhere': 836,\n"," 'yell': 837,\n"," 'simple': 838,\n"," 'wet': 839,\n"," 'machine': 840,\n"," 'usually': 841,\n"," 'nation': 842,\n"," 'flew': 843,\n"," 's': 844,\n"," 'poor': 845,\n"," 'guy': 846,\n"," 'race': 847,\n"," 'children': 848,\n"," 'except': 849,\n"," 'pencils': 850,\n"," 'famous': 851,\n"," 'third': 852,\n"," 'along': 853,\n"," 'glue': 854,\n"," 'women': 855,\n"," 'green': 856,\n"," 'poems': 857,\n"," 'court': 858,\n"," 'driving': 859,\n"," 'young': 860,\n"," 'cemetery': 861,\n"," 'dead': 862,\n"," 'vacuum': 863,\n"," 'dial': 864,\n"," 'yikes': 865,\n"," 'service': 866,\n"," 'desk': 867,\n"," 'gravity': 868,\n"," 'glass': 869,\n"," 'kidding': 870,\n"," 'teaching': 871,\n"," 'drove': 872,\n"," 'spot': 873,\n"," 'books': 874,\n"," 'add': 875,\n"," 'math': 876,\n"," 'taking': 877,\n"," 'ran': 878,\n"," 'hospital': 879,\n"," 'wipes': 880,\n"," 'faster': 881,\n"," 'student': 882,\n"," 'crosswalk': 883,\n"," 'speed': 884,\n"," 'fault': 885,\n"," 'hamburger': 886,\n"," 'worse': 887,\n"," 'flying': 888,\n"," 'wind': 889,\n"," 'anywhere': 890,\n"," 'stopped': 891,\n"," 'cart': 892,\n"," 'uses': 893,\n"," 'wins': 894,\n"," 'neighbors': 895,\n"," 'season': 896,\n"," 'sun': 897,\n"," 'goodness': 898,\n"," 'streets': 899,\n"," 'gas': 900,\n"," 'yard': 901,\n"," 'tells': 902,\n"," 'shows': 903,\n"," 'wants': 904,\n"," 'sit': 905,\n"," 'important': 906,\n"," 'follows': 907,\n"," 'works': 908,\n"," 'ugly': 909,\n"," 'strong': 910,\n"," 'interested': 911,\n"," 'caught': 912,\n"," 'met': 913,\n"," 'cook': 914,\n"," 'spit': 915,\n"," 'immediately': 916,\n"," 'door': 917,\n"," 'napkin': 918,\n"," 'foul': 919,\n"," 'holes': 920,\n"," 'lake': 921,\n"," 'practice': 922,\n"," 'tiger': 923,\n"," 'relax': 924,\n"," 'golf': 925,\n"," 'law': 926,\n"," 'breathe': 927,\n"," 'rest': 928,\n"," 'receipt': 929,\n"," 'article': 930,\n"," 'doors': 931,\n"," 'earthquakes': 932,\n"," 'florida': 933,\n"," 'october': 934,\n"," 'breakfast': 935,\n"," 'eggs': 936,\n"," 'traffic': 937,\n"," 'least': 938,\n"," 'huge': 939,\n"," 'during': 940,\n"," 'selling': 941,\n"," 'hurts': 942,\n"," 'bomb': 943,\n"," 'stuck': 944,\n"," 'soldiers': 945,\n"," 'band': 946,\n"," 'flip': 947,\n"," 'place': 948,\n"," 'become': 949,\n"," 'sell': 950,\n"," 'company': 951,\n"," 'd': 952,\n"," 'corporations': 953,\n"," 'dressing': 954,\n"," 'pasta': 955,\n"," 'bananas': 956,\n"," 'weight': 957,\n"," 'pound': 958,\n"," 'solution': 959,\n"," 'sent': 960,\n"," 'peanut': 961,\n"," 'corner': 962,\n"," 'apartment': 963,\n"," 'obama': 964,\n"," 'voters': 965,\n"," 'candidate': 966,\n"," 'election': 967,\n"," 'ads': 968,\n"," 'blood': 969,\n"," 'drops': 970,\n"," 'pale': 971,\n"," 'smoker': 972,\n"," 'rub': 973,\n"," 'brush': 974,\n"," 'picking': 975,\n"," 'attend': 976,\n"," 'degrees': 977,\n"," 'horrible': 978,\n"," 'hopefully': 979,\n"," 'clear': 980,\n"," 'closer': 981,\n"," 'cleaner': 982,\n"," 'changing': 983,\n"," 'impossible': 984,\n"," 'calling': 985,\n"," 'goodbye': 986,\n"," 'somewhere': 987,\n"," 'special': 988,\n"," 'noticed': 989,\n"," 'upset': 990,\n"," 'seriously': 991,\n"," 'deserved': 992,\n"," 'truth': 993,\n"," 'offered': 994,\n"," 'drawing': 995,\n"," 'painting': 996,\n"," 'often': 997,\n"," 'funniest': 998,\n"," 'laugh': 999,\n"," 'honestly': 1000,\n"," ...}"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["input_stc = input()\n","token_stc = input_stc.split()\n","encode_stc = tokenizer_qu.texts_to_sequences([token_stc])\n","pad_stc = pad_sequences(encode_stc, maxlen=21, padding=\"post\")\n","\n","en_out, en_hidden, en_cell = encoder_model.predict(pad_stc)\n","\n","predicted_seq = np.zeros((1,1))\n","predicted_seq[0, 0] = an_to_index['starttoken']\n","\n","decoded_stc = []\n","\n","while True:\n","    output_words, h, c = decoder_model.predict([predicted_seq, en_out, en_hidden, en_cell])\n","\n","    predicted_word = index_to_an[np.argmax(output_words[0,0])]\n","\n","    if predicted_word == 'endtoken' or len(decoded_stc)>= 20:\n","        break\n","\n","    decoded_stc.append(predicted_word)\n","\n","    predicted_seq = np.zeros((1,1))\n","    predicted_seq[0, 0] = np.argmax(output_words[0, 0])\n","\n","    en_hidden = h\n","    en_cell = c\n","\n","print(' '.join(decoded_stc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LU3XR4pG3nM","executionInfo":{"status":"ok","timestamp":1681670088560,"user_tz":-540,"elapsed":8406,"user":{"displayName":"hyun kim","userId":"01411993080668172019"}},"outputId":"4c9112f2-524e-4405-9b9c-46421e868973"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hi how are you\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 50ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 42ms/step\n","i am not a lot of the house\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tZilqvsXG3hX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"I1zsAdEJG3e4"},"execution_count":null,"outputs":[]}]}